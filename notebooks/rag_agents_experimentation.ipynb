{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f4c2b7",
   "metadata": {},
   "source": [
    "# RAG settings and Pinecone data uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbb9c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jorge/DocumentsWLS/Data_Science_Projects/Geospatial-similarity-combining-TDA-LLM-agents-and-MLOps-workflows\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57019960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables del archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# === Claves principales ===\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# === LangSmith config ===\n",
    "LANGSMITH_TRACING_V2 = os.getenv(\"LANGSMITH_TRACING_V2\", \"true\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\", \"RAG-Municipios\")\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = LANGSMITH_TRACING_V2\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f5704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "# Extract text from PDF files:\n",
    "\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2c72633",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_files(\"flask_app/data_RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3db58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objetcs, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(page_content=doc.page_content,\n",
    "            metadata={\"source\": src})\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c8c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ed0ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 26 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1500,\n",
    "        chunk_overlap = 100,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk\n",
    "\n",
    "texts_chunk = text_split(minimal_docs)\n",
    "\n",
    "print(f\"Split blog post into {len(texts_chunk)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87c4edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 23 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "def text_split_token(minimal_docs):\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=500,         # n煤mero de tokens por chunk\n",
    "        chunk_overlap=50,       # superposici贸n entre chunks\n",
    "        encoding_name=\"cl100k_base\"  # tokenizer compatible con GPT-5-nano / GPT-4-turbo\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk\n",
    "\n",
    "texts_chunk = text_split_token(minimal_docs)\n",
    "\n",
    "print(f\"Split blog post into {len(texts_chunk)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15a160b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-distilroberta-v1\"\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c05baf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cuda'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.model_kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a1ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import ServerlessSpec\n",
    "from pinecone import Pinecone\n",
    "index_name = \"research-assistant\"\n",
    "\n",
    "pc = Pinecone()\n",
    "# create the index if it doesn't exist\n",
    "if index_name not in pc.list_indexes():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # replace with your embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# connect to the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea9c9a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7246882a5090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunk,\n",
    "    embedding=embedding,\n",
    "    index_name= index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc94ee1",
   "metadata": {},
   "source": [
    "# AI Agents creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07b9f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# embbeded each chunk and upsert the embeddings into your pinecone index\n",
    "index_name = \"research-assistant\"\n",
    "\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "559e7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools import tool\n",
    "\n",
    "# RAG tool\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information from internal documentation or PDFs.\n",
    "\n",
    "    Use this tool when the user asks about technical or implementation details\n",
    "    within the project. It performs a semantic similarity search in the\n",
    "    document embeddings to extract the most relevant passages.\n",
    "\n",
    "    Example queries:\n",
    "    - \"Which library was used for VietorisRips?\"\n",
    "    - \"Was DVC used for reproducibility?\"\n",
    "    - \"Explain how GPU support was integrated with giotto-tda.\"\n",
    "    \"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant context found in the documentation.\", []\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\" **Source:** {doc.metadata}\\n{doc.page_content}\" for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "# DuckDuckGo tool\n",
    "@tool\n",
    "def duckduckgo_search(query: str) -> str:\n",
    "    \"\"\"Search the web using DuckDuckGo.\n",
    "\n",
    "    Use this tool for general information retrieval from online sources such as news,\n",
    "    research sites, and Wikipedia. Limit results to the top 3 most relevant hits.\n",
    "\n",
    "    Example queries:\n",
    "    - \"Recent research on persistent homology\"\n",
    "    - \"Farms cultivating lettuce near Aguascalientes\"\n",
    "    \"\"\"\n",
    "    results = DuckDuckGoSearchRun().run(query)\n",
    "    if not results:\n",
    "        return \"No results found.\"\n",
    "    top_results = results[:3]\n",
    "    return \"\\n\".join(f\" {r['title']}: {r['href']}\" for r in top_results)\n",
    "\n",
    "# Crear el client con Taviliy MCP\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"tavily\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"https://mcp.tavily.com/mcp\",\n",
    "            \"headers\": {\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Obtener todas las tools\n",
    "tools_mcp = await client.get_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "context_prompt = (\n",
    "    \"You are a specialized **retrieval assistant** focused on providing accurate and contextually relevant \"\n",
    "    \"answers from local project documentation, PDFs, or embedded sources. \"\n",
    "    \"You have access to a retrieval tool that searches within vectorized documents. \"\n",
    "    \"When answering, use this tool to ground your responses in factual information extracted from the data. \"\n",
    "    \"If you cannot find relevant context, clearly state that no related information was found. \"\n",
    "    \"Keep answers **brief, concise, technical, and well-structured**, using bullet points when helpful. \"\n",
    "    \"Include all relevant details necessary for a complete answer.\"\n",
    ")\n",
    "model_context = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning_effort='low'\n",
    ")\n",
    "\n",
    "context_agent = create_agent(\n",
    "    model=model_context,\n",
    "    tools=[retrieve_context],\n",
    "    system_prompt=context_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e929583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_D7BUwxF43zJ3GSaAFcuNmvtu)\n",
      " Call ID: call_D7BUwxF43zJ3GSaAFcuNmvtu\n",
      "  Args:\n",
      "    query: What MLOPS tools were used?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      " **Source:** {'source': 'flask_app/data_RAG/An谩lisis Topol贸gico con GPU y Visualizaci贸n Interactiva Basada en GenAI.pdf'}\n",
      "An谩lisis Topol贸gico con GPU y Visualizaci贸n Interactiva \n",
      "Basada en GenAI \n",
      " \n",
      "Abstract \n",
      "Este trabajo presenta un flujo t茅cnico reproducible para el An谩lisis Topol贸gico de Datos \n",
      "(TDA) enfocado en la caracterizaci贸n y comparaci贸n de municipios mediante indicadores \n",
      "clim谩ticos, edafol贸gicos y de uso de suelo. La propuesta integra c贸mputo acelerado por \n",
      "GPU a trav茅s de una adaptaci贸n de ripser++ como backend de alto rendimiento, lo que \n",
      "permite reducir significativamente el tiempo de c谩lculo de homolog铆a persistente dentro \n",
      "del complejo de VietorisRips. \n",
      "El sistema incorpora una arquitectura modular orientada a objetos (OOP) y un pipeline \n",
      "controlado con DVC (Data Version Control), garantizando trazabilidad y replicabilidad en \n",
      "cada etapa experimental. Adem谩s, se desarroll贸 una interfaz anal铆tica interactiva \n",
      "soportada por agentes basados en el Model Context Protocol (MCP) y un flujo RAG \n",
      "(Retrieval-Augmented Generation), capaces de explicar din谩micamente el modelo y sus \n",
      "resultados.\n",
      "\n",
      " **Source:** {'source': 'flask_app/data_RAG/An谩lisis Topol贸gico con GPU y Visualizaci贸n Interactiva Basada en GenAI.pdf'}\n",
      "Conclusi贸n ................................ ................................ ................................ ............. 16 \n",
      " \n",
      " \n",
      "Introducci贸n \n",
      "Este trabajo presenta un flujo t茅cnico para el An谩lisis Topol贸gico de Datos (TDA) que integra \n",
      "c贸mputo acelerado por GPU y herramientas modernas de reproducibilidad cient铆fica. \n",
      "El desarrollo constituye una extensi贸n de un trabajo previo, al cual se le incorpor贸 una \n",
      "arquitectura modular orientada a objetos (OOP) y la capacidad de emplear GPU para \n",
      "optimizar el tiempo de construcci贸n del complejo de VietorisRips, utilizando la librer铆a \n",
      "ripser++  de alto rendimiento. \n",
      "A diferencia de versiones anteriores, se elimin贸 la base de datos relacional y se reconstruy贸 la \n",
      "aplicaci贸n desde cero, con el objetivo de integrar Agentes basados en el Model Context \n",
      "Protocol (MCP) capaces de explicar din谩micamente el proyecto mediante un pipeline RAG \n",
      "(Retrieval-Augmented Generation).\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "- DVC (Data Version Control): used to ensure reproducibility and traceability across experiment steps.\n",
      "- No additional MLOps tools are explicitly mentioned in the provided document.\n"
     ]
    }
   ],
   "source": [
    "query = \"What MLOPS tools were used?\"\n",
    "\n",
    "for step in context_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "):\n",
    "    for update in step.values():\n",
    "        for message in update.get(\"messages\", []):\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "862625ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "web_prompt = (\n",
    "    \"You are a **web research assistant** specialized in finding relevant and reliable information from the Internet. \"\n",
    "    \"You have access to multiple web tools (e.g., DuckDuckGo, Tavily, Wikipedia MCP). \"\n",
    "    \"Use these tools whenever necessary to support your answer with external references. \"\n",
    "    \"If the search results do not fully address the user's question, end your response with the sentence: \"\n",
    "    \"'The data obtained might not answer the user's question entirely.'. \"\n",
    "    \"Focus on clarity, conciseness, and factual accuracy, and explain the answer as briefly as possible. \"\n",
    "\n",
    ")\n",
    "model_web = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning_effort='low'\n",
    ")\n",
    "\n",
    "web_agent = create_agent(\n",
    "    model=model_web,\n",
    "    tools=[duckduckgo_search]+tools_mcp,\n",
    "    system_prompt=web_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7327dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "John Fitzgerald Kennedy (JFK) was the 35th President of the United States, serving from 1961 until his assassination in 1963.\n",
      "\n",
      "Key points:\n",
      "- Party: Democrat\n",
      "- Region: Massachusetts, U.S. Senator before becoming president\n",
      "- Background: Naval officer in World War II; author and public figure\n",
      "- Major themes: The New Frontier (his domestic agenda), civil rights momentum, and Cold War crises\n",
      "- Notable events: Cuban Missile Crisis (1962), Bay of Pigs invasion (1961), space program momentum (Space Race)\n",
      "- Assassination: Killed on November 22, 1963, in Dallas, Texas\n",
      "\n",
      "If you meant a different JFK (e.g., John F. Kennedy Jr. or another person with those initials), tell me and I can provide details.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who was JFK?\"\n",
    "\n",
    "for step in web_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "):\n",
    "    for update in step.values():\n",
    "        for message in update.get(\"messages\", []):\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a792746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def context_retriever(request: str) -> str:\n",
    "    \"\"\"Retrieve answers from local project documentation.\"\"\"\n",
    "    result = context_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": request}]})\n",
    "    response = result[\"messages\"][-1] if isinstance(result, dict) else result\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_retriever(request: str) -> str:\n",
    "    \"\"\"Retrieve information from the web using Tavily, DuckDuckGo, or Wikipedia MCP.\"\"\"\n",
    "    result = web_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": request}]})\n",
    "    response = result[\"messages\"][-1] if isinstance(result, dict) else result\n",
    "    return response.content if hasattr(response, \"content\") else str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74d5363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "supervisor_prompt = (\n",
    "    \"You are an intelligent orchestrator responsible for coordinating specialized agents and tools. \"\n",
    "    \"Before taking action, check the available context and metadata to determine whether the user's question is already fully answered. \"\n",
    "    \"If sufficient context exists, respond directly without using tools. \"\n",
    "    \"Otherwise, select the appropriate tools: use context_retriever for project or document-related questions, \"\n",
    "    \"and web_retriever for external or general knowledge. \"\n",
    "    \"Plan and execute multi-step tasks logically, integrating tool outputs into a single, coherent answer. \"\n",
    "    \"Always be concise, brief, factual, and focused on the users intent.\"\n",
    ")\n",
    "model_supervisor = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning_effort='medium'\n",
    ")\n",
    "checkpointer = InMemorySaver()\n",
    "summarization_prompt = (\n",
    "    \"You are a summarization assistant. \"\n",
    "    \"Read the recent conversation messages and condense them into a brief, clear summary without losing too much info. \"\n",
    "    \"Focus on the key facts, context, and questions that are relevant to answering the user. \"\n",
    "    \"Avoid unnecessary details or repetitions. \"\n",
    "    \"Format the summary in a structured way so that it can be easily consumed by another agent. \"\n",
    "    \"Keep it factual, and as precise as possible.\"\n",
    ")\n",
    "# Inicializa el LLM\n",
    "model_summary = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning_effort='low')\n",
    "\n",
    "supervisor_agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[context_retriever, web_retriever],\n",
    "    system_prompt=supervisor_prompt,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model_summary,          # usa un modelo m谩s ligero para resumir ####PROBAR CON \"gpt-4-nano\", creo que es m谩s r谩pido\n",
    "            max_tokens_before_summary=2000,  # resume cuando pasa los 2000 tokens\n",
    "            messages_to_keep=2, #mantiene solo los 煤ltimos 2 mensajes\n",
    "            summary_prompt=summarization_prompt)\n",
    "    ],\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8d463ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza un string:\n",
    "    - Convierte a min煤sculas\n",
    "    - Quita s铆mbolos innecesarios (excepto ? y !)\n",
    "    - Elimina espacios m煤ltiples y caracteres innecesarios\n",
    "    \"\"\"\n",
    "    # Convertir a min煤sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Quitar s铆mbolos que no sean letras, n煤meros, espacios, ?, !\n",
    "    text = re.sub(r\"[^a-z0-9谩茅铆贸煤眉帽\\s?!.,;:%+*'\\\"()\\[\\]-]\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Reemplazar m煤ltiples espacios por uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7749877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola! c贸mo est谩s? esto es un test: 100%*4 funcional.\n"
     ]
    }
   ],
   "source": [
    "texto = \"隆Hola! 驴C贸mo est谩s? Esto es un test: 100%*4 funcional.\"\n",
    "print(clean_text(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "297e4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  context_retriever (call_g8cEI8GVdEDa5yGdxgBgDf0B)\n",
      " Call ID: call_g8cEI8GVdEDa5yGdxgBgDf0B\n",
      "  Args:\n",
      "    request: project persistence diagrams: how were they calculated?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: context_retriever\n",
      "\n",
      "- Method: Persistent homology computed via Ripser++ (GPU-accelerated version).\n",
      "- Pipeline: Integrated within giotto-tda workflow; results fed into persistence diagrams.\n",
      "- Matrix/type: Based on a distance-based filtration (VietorisRips) as implemented by Ripser++.\n",
      "- Rationale: Ripser++ offered substantial time reductions while remaining compatible with the giotto-tda ecosystem.\n",
      "- Additional context: The approach sits inside a broader TDA+GPU reproducibility framework described in the document.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Se calcularon as铆:\n",
      "\n",
      "- M茅todo: Persistent homology con Ripser++ (versi贸n GPU).\n",
      "- Pipeline: integrado dentro del flujo de trabajo de giotto-tda; los diagramas se obtienen a partir de ese pipeline.\n",
      "- Matriz/Filtraci贸n: filtraci贸n basada en distancias, VietorisRips (implementada por Ripser++).\n",
      "- Raz贸n: Ripser++ ofrece grandes reducciones de tiempo y es compatible con giotto-tda.\n",
      "- Contexto adicional: forma parte del marco TDA+GPU para reproducibilidad.\n",
      "\n",
      "Si quieres, te comparto los pasos o un snippet de c贸digo para reproducirlo en tu entorno.\n"
     ]
    }
   ],
   "source": [
    "query = \"C贸mo se calcularon los diagramas de persistencia en el proyecto?\"\n",
    "query = clean_text(query)\n",
    "config = make_config_from_query(query)\n",
    "\n",
    "for step in supervisor_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    config=config,\n",
    "):\n",
    "    for update in step.values():\n",
    "        if not update:  #  Evita errores con None\n",
    "            continue\n",
    "        for message in update.get(\"messages\", []):\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import session\n",
    "import uuid\n",
    "\n",
    "if \"user_id\" not in session:\n",
    "    session[\"user_id\"] = str(uuid.uuid4())\n",
    "user_id = session[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3db5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import inspect\n",
    "\n",
    "# Inicializa el LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning_effort='low'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c485271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n"
     ]
    }
   ],
   "source": [
    "print(llm.reasoning_effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715d5570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.agents.middleware.summarization.SummarizationMiddleware at 0x752f430305d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "summarization_prompt = (\n",
    "    \"You are a summarization assistant. \"\n",
    "    \"Read the recent conversation messages and condense them into a brief, clear summary. \"\n",
    "    \"Focus on the key facts, context, and questions that are relevant to answering the user. \"\n",
    "    \"Avoid unnecessary details or repetitions. \"\n",
    "    \"Format the summary in a structured way so that it can be easily consumed by another agent. \"\n",
    "    \"Keep it short, factual, and precise.\"\n",
    ")\n",
    "SummarizationMiddleware(\n",
    "            model=llm,          # usa un modelo m谩s ligero para resumir ####PROBAR CON \"gpt-4-nano\", creo que es m谩s r谩pido\n",
    "            max_tokens_before_summary=2000,  # resume cuando pasa los 2000 tokens\n",
    "            messages_to_keep=2, #mantiene solo los 煤ltimos 2 mensajes\n",
    "            summary_prompt=summarization_prompt\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0622d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDAGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
