{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d3f5e5",
   "metadata": {},
   "source": [
    "# RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43935104",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/langchain/supervisor\n",
    "https://docs.langchain.com/oss/python/langchain/rag\n",
    "https://docs.langchain.com/oss/python/langgraph/agentic-rag\n",
    "https://docs.langchain.com/oss/python/langchain/mcp\n",
    "https://docs.langchain.com/oss/python/langgraph/workflows-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2294d02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jorge/DocumentsWLS/Data_Science_Projects/Geospatial-similarity-combining-TDA-LLM-agents-and-MLOps-workflows\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078aa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf98d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574078ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d104d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import ServerlessSpec\n",
    "from pinecone import Pinecone\n",
    "index_name = \"research-assistant\"\n",
    "\n",
    "pc = Pinecone()\n",
    "# create the index if it doesn't exist\n",
    "if index_name not in pc.list_indexes():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # replace with your embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb472408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"flask_app/data_RAG/Análisis Topológico con GPU y Visualización Interactiva Basada en GenAI.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eff6e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 39 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d881a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['94d2fcf5-1758-4893-8abb-77e259ef6c30', 'e04c9193-e125-447f-9020-720c49a32193', '24204ad6-9d87-4d5e-9ca8-38f362f6156f']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06877b0",
   "metadata": {},
   "source": [
    "## Query pensada, el modelo decide si usar el tool o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cafbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdddfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a pdf. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "397b4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the formula for wassertein distance?\n",
      "\n",
      "Give the answer as concise as possible\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are the standard formulas (p-Wasserstein distance):\n",
      "\n",
      "- General (p ≥ 1):\n",
      "  W_p(μ, ν) = (inf_{γ ∈ Π(μ, ν)} ∫∫ ||x − y||^p dγ(x,y))^{1/p},\n",
      "  where Π(μ, ν) is the set of couplings with marginals μ and ν.\n",
      "\n",
      "- Discrete case (empirical measures with atoms x_i, y_j and weights a_i, b_j):\n",
      "  W_p^p(μ, ν) = min_{γ ≥ 0} ∑_{i,j} γ_{ij} ||x_i − y_j||^p\n",
      "  subject to ∑_j γ_{ij} = a_i and ∑_i γ_{ij} = b_j.\n",
      "\n",
      "- 1D closed form (if μ, ν on the real line with CDFs F, G):\n",
      "  W_p(μ, ν) = (∫_0^1 |F^{-1}(t) − G^{-1}(t)|^p dt)^{1/p}.\n",
      "\n",
      "- Special case p = 1 (W_1) dual form:\n",
      "  W_1(μ, ν) = sup_{Lip(f) ≤ 1} ∫ f dμ − ∫ f dν.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the formula for wassertein distance?\\n\\n\"\n",
    "    \"Give the answer as concise as possible\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "159be5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the formula for wassertein distance?\\n\\nGive the answer in 5 rows at most', additional_kwargs={}, response_metadata={}, id='ebcbba54-6c7b-40c6-9943-0571e38a8cd0'),\n",
       "  AIMessage(content='1) W_p(μ,ν) = (inf_{γ∈Π(μ,ν)} ∫_{X×X} d(x,y)^p dγ(x,y))^{1/p}, p≥1\\n\\n2) Π(μ,ν) = {γ≥0: γ(A×X)=μ(A), γ(X×A)=ν(A)}\\n\\n3) W_1(μ,ν) = sup_{f: Lip(f)≤1} [∫ f dμ − ∫ f dν] (Kantorovich–Rubinstein)\\n\\n4) Discrete: μ=∑_i p_i δ_{x_i}, ν=∑_j q_j δ_{y_j} ⇒\\n   W_p(μ,ν) = (min_{Γ≥0} ∑_{i,j} Γ_{ij} d(x_i,y_j)^p)^{1/p},\\n   with ∑_j Γ_{ij}=p_i, ∑_i Γ_{ij}=q_j\\n\\n5) Assumes μ,ν have finite p-th moments on a metric space (X,d).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2165, 'prompt_tokens': 170, 'total_tokens': 2335, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1920, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CZpDNc2w2qwhQmqQurNUGpnyGtK3N', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--aecb71e5-9592-4271-8de9-892639e26d23-0', usage_metadata={'input_tokens': 170, 'output_tokens': 2165, 'total_tokens': 2335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1920}})]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185a62b",
   "metadata": {},
   "source": [
    "## Query que siempre se hace, el contexto siempre se añade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef67f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af373eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3119044",
   "metadata": {},
   "source": [
    "## Memoria conversacional eliminar memoria muy vieja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5662d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import after_model\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "@after_model\n",
    "def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n",
    "    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    return None\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5-nano\",\n",
    "    tools=[],\n",
    "    system_prompt=\"Please be concise and to the point.\",\n",
    "    middleware=[delete_old_messages],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2926b",
   "metadata": {},
   "source": [
    "## Resumir la conversación anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae88397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "        )\n",
    "    ],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\"\"\"\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "Your name is Bob!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d25604",
   "metadata": {},
   "source": [
    "# Custom RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55985ff",
   "metadata": {},
   "source": [
    "## Crear un retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133afd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"project_documentation\",\n",
    "    \"Search and return information about the project\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool.invoke({\"query\": \"types of reward hacking\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393eb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "response_model = init_chat_model(\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])  \n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f9449",
   "metadata": {},
   "source": [
    "## Evaluar si la respuesta es adecuada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):  \n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "grader_model = init_chat_model(\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(  \n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540e6ec",
   "metadata": {},
   "source": [
    "### irrelevant doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ced25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06910d23",
   "metadata": {},
   "source": [
    "### Relevant doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeb620",
   "metadata": {},
   "source": [
    "## Rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f51a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95660e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0b531",
   "metadata": {},
   "source": [
    "## Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42752470",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712902a0",
   "metadata": {},
   "source": [
    "## Unir todo en un grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693288f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f27a7",
   "metadata": {},
   "source": [
    "# Multi-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32653491",
   "metadata": {},
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2fccb",
   "metadata": {},
   "source": [
    "## define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def create_calendar_event(\n",
    "    title: str,\n",
    "    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\n",
    "    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\n",
    "    attendees: list[str],  # email addresses\n",
    "    location: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n",
    "    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n",
    "    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def send_email(\n",
    "    to: list[str],  # email addresses\n",
    "    subject: str,\n",
    "    body: str,\n",
    "    cc: list[str] = []\n",
    ") -> str:\n",
    "    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n",
    "    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n",
    "    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_available_time_slots(\n",
    "    attendees: list[str],\n",
    "    date: str,  # ISO format: \"2024-01-15\"\n",
    "    duration_minutes: int\n",
    ") -> list[str]:\n",
    "    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n",
    "    # Stub: In practice, this would query calendar APIs\n",
    "    return [\"09:00\", \"14:00\", \"16:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393da9a",
   "metadata": {},
   "source": [
    "## Create specialized sub-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44785f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "CALENDAR_AGENT_PROMPT = (\n",
    "    \"You are a calendar scheduling assistant. \"\n",
    "    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n",
    "    \"into proper ISO datetime formats. \"\n",
    "    \"Use get_available_time_slots to check availability when needed. \"\n",
    "    \"Use create_calendar_event to schedule events. \"\n",
    "    \"Always confirm what was scheduled in your final response.\"\n",
    ")\n",
    "\n",
    "calendar_agent = create_agent(\n",
    "    model,\n",
    "    tools=[create_calendar_event, get_available_time_slots],\n",
    "    system_prompt=CALENDAR_AGENT_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\n",
    "\n",
    "for step in calendar_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "):\n",
    "    for update in step.values():\n",
    "        for message in update.get(\"messages\", []):\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8419078",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_AGENT_PROMPT = (\n",
    "    \"You are an email assistant. \"\n",
    "    \"Compose professional emails based on natural language requests. \"\n",
    "    \"Extract recipient information and craft appropriate subject lines and body text. \"\n",
    "    \"Use send_email to send the message. \"\n",
    "    \"Always confirm what was sent in your final response.\"\n",
    ")\n",
    "\n",
    "email_agent = create_agent(\n",
    "    model,\n",
    "    tools=[send_email],\n",
    "    system_prompt=EMAIL_AGENT_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d357d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Send the design team a reminder about reviewing the new mockups\"\n",
    "\n",
    "for step in email_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "):\n",
    "    for update in step.values():\n",
    "        for message in update.get(\"messages\", []):\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e94373",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def schedule_event(request: str) -> str:\n",
    "    \"\"\"Schedule calendar events using natural language.\n",
    "\n",
    "    Use this when the user wants to create, modify, or check calendar appointments.\n",
    "    Handles date/time parsing, availability checking, and event creation.\n",
    "\n",
    "    Input: Natural language scheduling request (e.g., 'meeting with design team\n",
    "    next Tuesday at 2pm')\n",
    "    \"\"\"\n",
    "    result = calendar_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    })\n",
    "    return result[\"messages\"][-1].text\n",
    "\n",
    "\n",
    "@tool\n",
    "def manage_email(request: str) -> str:\n",
    "    \"\"\"Send emails using natural language.\n",
    "\n",
    "    Use this when the user wants to send notifications, reminders, or any email\n",
    "    communication. Handles recipient extraction, subject generation, and email\n",
    "    composition.\n",
    "\n",
    "    Input: Natural language email request (e.g., 'send them a reminder about\n",
    "    the meeting')\n",
    "    \"\"\"\n",
    "    result = email_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    })\n",
    "    return result[\"messages\"][-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERVISOR_PROMPT = (\n",
    "    \"You are a helpful personal assistant. \"\n",
    "    \"You can schedule calendar events and send emails. \"\n",
    "    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n",
    "    \"When a request involves multiple actions, use multiple tools in sequence.\"\n",
    ")\n",
    "\n",
    "supervisor_agent = create_agent(\n",
    "    model,\n",
    "    tools=[schedule_event, manage_email],\n",
    "    system_prompt=SUPERVISOR_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d497727",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Schedule a team standup for tomorrow at 9am\"\n",
    "\n",
    "for step in supervisor_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "):\n",
    "    for update in step.values():\n",
    "        for message in update.get(\"messages\", []):\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20bff9",
   "metadata": {},
   "source": [
    "# MCP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaeede9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDAGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
